{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClipCard Agent Test: Policy/Trust & Safety",
    "\n",
    "**Purpose:** This notebook tests whether an AI agent (LLM) can correctly parse, validate, and reason about ClipCards in the Policy and Trust & Safety domain.",
    "\n",
    "It evaluates:",
    "1. Can the agent extract kill criteria (rollback thresholds) from a policy pilot ClipCard?",
    "2. Can it determine if a given set of metrics (scenario) would trigger the criteria?",
    "3. Can it suggest appropriate authority windows for policy rollouts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json",
    "import pandas as pd",
    "",
    "# Load example ClipCard",
    "try:",
    "    with open('../../examples/policy-ts-test.clipcard.json', 'r') as f:",
    "        test_card = json.load(f)",
    "    print(\"Loaded test ClipCard:")",
    "    print(json.dumps(test_card, indent=2))",
    "except FileNotFoundError:",
    "    print(\"No test ClipCard found.")",
    "    test_card = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Extract Kill Criteria",
    "\n",
    "Can we programmatically parse the rollback thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_card:",
    "    kill_criteria = test_card.get('kill_criteria', [])",
    "    print(f\"Found {len(kill_criteria)} kill criteria:\n\")",
    "    for i, criterion in enumerate(kill_criteria, 1):",
    "        print(f\"{i}. Condition: {criterion['condition']}\")",
    "        print(f\"   Action: {criterion['action']}\")",
    "else:",
    "    print(\"No ClipCard available for testing.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Scenario Evaluation",
    "\n",
    "Test if weekly metrics from a policy pilot would trigger the rollback criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios for policy metrics",
    "scenarios = [",
    "    {",
    "        \"name\": \"Normal Week\",",
    "        \"verified_wrongful_rate_per_10k\": 0.5,",
    "        \"incident_severity\": 0,",
    "        \"expected_trigger\": false",
    "    },",
    "    {",
    "        \"name\": \"High Wrongful Rate\",",
    "        \"verified_wrongful_rate_per_10k\": 1.2,",
    "        \"incident_severity\": 1,",
    "        \"expected_trigger\": true",
    "    },",
    "    {",
    "        \"name\": \"Major Incident\",",
    "        \"verified_wrongful_rate_per_10k\": 0.6,",
    "        \"incident_severity\": 2,",
    "        \"expected_trigger\": true",
    "    },",
    "    {",
    "        \"name\": \"Wrongful Rate Approaching Limit\",",
    "        \"verified_wrongful_rate_per_10k\": 0.79,",
    "        \"incident_severity\": 0,",
    "        \"expected_trigger\": false",
    "    }",
    "],",
    "",
    "def evaluate_policy_scenarios(card, scenarios):",
    "    results = []",
    "    criteria = card.get('kill_criteria', [])",
    "    ",
    "    for scenario in scenarios:",
    "        triggered = False",
    "        for rule in criteria:",
    "            condition = rule['condition']",
    "            if 'verified_wrongful_rate_per_10k' in condition and scenario['verified_wrongful_rate_per_10k'] >= 0.8:",
    "                triggered = True",
    "            if 'incident_severity' in condition and scenario['incident_severity'] >= 2:",
    "                triggered = True",
    "        ",
    "        match = triggered == scenario['expected_trigger']",
    "        results.append({",
    "            'Scenario': scenario['name'],",
    "            'Triggered': triggered,",
    "            'Expected': scenario['expected_trigger'],",
    "            'Match': '✓' if match else '✗'",
    "        })",
    "    return pd.DataFrame(results)",
    "",
    "if test_card:",
    "    df_results = evaluate_policy_scenarios(test_card, scenarios)",
    "    print(\"Evaluating policy scenarios against the test ClipCard\n")",
    "    print(df_results.to_string(index=False))",
    "    print(f\"\nTest Accuracy: {df_results['Match'].value_counts().get('✓', 0)}/{len(df_results)}")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Authority Window Recommendations",
    "\n",
    "Generate appropriate authority windows for a new policy pilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_policy_authority_window(impact, uncertainty, reversibility):",
    "    \"\"\"Recommends a policy-specific authority window based on risk.\"\"\"",
    "    risk_score = impact * uncertainty",
    "    ",
    "    if risk_score >= 20 or reversibility >= 4 or impact >= 5:",
    "        return {",
    "            \"scope_limit\": \"1% of a single, non-critical market\",",
    "            \"time_limit\": \"14 days\",",
    "            \"auto_pause\": true,",
    "            \"justification\": \"High risk to user rights or safety requires a very limited and short pilot.\"",
    "        }",
    "    elif risk_score >= 12:",
    "        return {",
    "            \"scope_limit\": \"10% of a single market\",",
    "            \"time_limit\": \"30 days\",",
    "            \"auto_pause\": true,",
    "            \"justification\": \"Medium risk allows for a controlled, single-market pilot.\"",
    "        }",
    "    else:",
    "        return {",
    "            \"scope_limit\": \"Global rollout, excluding sensitive markets\",",
    "            \"time_limit\": \"90 days until mandatory review\",",
    "            \"auto_pause\": false,",
    "            \"justification\": \"Lower risk allows for a broader rollout with a scheduled review.\"",
    "        }",
    "",
    "# Test with example policy scenarios",
    "test_cases = [",
    "    {\"name\": \"New hate speech classifier\", \"impact\": 5, \"uncertainty\": 4, \"reversibility\": 3},",
    "    {\"name\": \"Minor change to spam filter\", \"impact\": 2, \"uncertainty\": 2, \"reversibility\": 1},",
    "    {\"name\": \"New policy on synthetic media\", \"impact\": 4, \"uncertainty\": 5, \"reversibility\": 4}",
    "",
    "print(\"Policy Authority Window Recommendations:\n")",
    "for case in test_cases:",
    "    rec = recommend_policy_authority_window(case['impact'], case['uncertainty'], case['reversibility'])",
    "    print(f\"{case['name']} (I={case['impact']}, U={case['uncertainty']}, R={case['reversibility']}):\")",
    "    print(f\"  Scope: {rec['scope_limit']}\")",
    "    print(f\"  Duration: {rec['time_limit']}\")",
    "    print(f\"  Auto-pause: {rec['auto_pause']}\")",
    "    print(f\"  Reason: {rec['justification']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}