{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# ClipCard Simulation: DevOps/SRE\n",
        "\n",
        "**Purpose:** This notebook runs a Monte Carlo simulation to estimate the effectiveness of ClipCard parameters for service deployments.\n",
        "\n",
        "It simulates deployments and checks if randomly generated metrics (like `error_rate` or `latency`) would trigger the `kill_criteria` defined in a ClipCard. This helps quantify the risk reduction provided by the card's specific configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Load example ClipCards from the examples or data directory\n",
        "try:\n",
        "    with open('../../examples/test.clipcard.json', 'r') as f:\n",
        "        example_card = json.load(f)\n",
        "    print(f\"Loaded example ClipCard: {example_card['id']}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No example ClipCard found. Using synthetic data.\")\n",
        "    example_card = {\n",
        "        \"id\": \"SIM-DEVOPS-001\",\n",
        "        \"kill_criteria\": [\n",
        "            {\"condition\": \"error_rate > 0.01\", \"action\": \"Rollback immediately\"},\n",
        "            {\"condition\": \"latency_p99 > 500\", \"action\": \"Reduce traffic to 1%\"}\n",
        "        ],\n",
        "        \"authority_window\": {\"scope_limit\": \"5% deployment\", \"time_limit\": \"30d\"}\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "### Simulation Parameters\n",
        "\n",
        "Configure simulation runs and metric distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulation configuration\n",
        "N_SIMULATIONS = 1000  # Number of deployment simulations\n",
        "COHORT_SIZE = 0.05    # 5% of traffic\n",
        "BASE_ERROR_RATE = 0.003  # Base error rate\n",
        "BASE_LATENCY_P99 = 250   # Base p99 latency in ms\n",
        "\n",
        "print(f\"Running {N_SIMULATIONS} simulations...\")\n",
        "print(f\"Deployment size: {COHORT_SIZE*100}%\")\n",
        "print(f\"Base error rate: {BASE_ERROR_RATE}\")\n",
        "print(f\"Base p99 latency: {BASE_LATENCY_P99}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "### Monte Carlo Simulation\n",
        "\n",
        "Run simulations and track kill criteria triggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for i in range(N_SIMULATIONS):\n",
        "    # Simulate metrics for a 24-hour period\n",
        "    requests = np.random.poisson(100000)  # Number of requests\n",
        "    \n",
        "    # Simulate error rate (Beta distribution with noise)\n",
        "    error_rate = np.random.beta(2, 500) + BASE_ERROR_RATE\n",
        "    \n",
        "    # Simulate p99 latency (Gamma distribution)\n",
        "    latency_p99 = np.random.gamma(2, BASE_LATENCY_P99/2)\n",
        "    \n",
        "    # Check kill criteria\n",
        "    error_trigger = error_rate > 0.01\n",
        "    latency_trigger = latency_p99 > 500\n",
        "    any_trigger = error_trigger or latency_trigger\n",
        "    \n",
        "    results.append({\n",
        "        'sim_id': i,\n",
        "        'requests': requests,\n",
        "        'error_rate': error_rate,\n",
        "        'latency_p99': latency_p99,\n",
        "        'error_trigger': error_trigger,\n",
        "        'latency_trigger': latency_trigger,\n",
        "        'any_trigger': any_trigger\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(f\"\nSimulation complete!\")\n",
        "print(f\"Error rate triggers: {df['error_trigger'].sum()} ({df['error_trigger'].mean()*100:.1f}%)\")\n",
        "print(f\"Latency triggers: {df['latency_trigger'].sum()} ({df['latency_trigger'].mean()*100:.1f}%)\")\n",
        "print(f\"Any trigger: {df['any_trigger'].sum()} ({df['any_trigger'].mean()*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "### Visualization\n",
        "\n",
        "Plot trigger distributions and risk reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Error rate distribution\n",
        "axes[0].hist(df['error_rate'], bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(0.01, color='red', linestyle='--', label='Kill threshold')\n",
        "axes[0].set_xlabel('Error Rate')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Error Rate Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Latency distribution\n",
        "axes[1].hist(df['latency_p99'], bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[1].axvline(500, color='red', linestyle='--', label='Kill threshold')\n",
        "axes[1].set_xlabel('P99 Latency (ms)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Latency Distribution')\n",
        "axes[1].legend()\n",
        "\n",
        "# Trigger summary\n",
        "trigger_counts = [\n",
        "    df['error_trigger'].sum(),\n",
        "    df['latency_trigger'].sum(),\n",
        "    df['any_trigger'].sum()\n",
        "]\n",
        "trigger_labels = ['Error Rate', 'Latency', 'Any Trigger']\n",
        "axes[2].bar(trigger_labels, trigger_counts, alpha=0.7, edgecolor='black')\n",
        "axes[2].set_ylabel('Number of Triggers')\n",
        "axes[2].set_title(f'Kill Criteria Triggers (n={N_SIMULATIONS})')\n",
        "axes[2].axhline(N_SIMULATIONS * 0.05, color='orange', linestyle='--', label='5% threshold')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\n** Interpretation **\")\n",
        "print(f\"In {N_SIMULATIONS} simulated 24h periods with a {COHORT_SIZE*100}% deployment:\")\n",
        "print(f\"- Kill criteria would trigger {df['any_trigger'].mean()*100:.1f}% of the time\")\n",
        "print(f\"- This suggests the authority window provides {(1-df['any_trigger'].mean())*100:.1f}% safe operation probability\")\n",
        "print(f\"- Estimated incident prevention: {df['any_trigger'].sum()} potential incidents caught early\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
