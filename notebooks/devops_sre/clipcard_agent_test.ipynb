{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# ClipCard Agent Test: DevOps/SRE\n",
        "\n",
        "**Purpose:** This notebook tests whether an AI agent (LLM) can correctly parse, validate, and reason about ClipCards in the DevOps/SRE domain.\n",
        "\n",
        "It evaluates:\n",
        "1. Can the agent extract kill criteria from a ClipCard?\n",
        "2. Can it determine if a given scenario would trigger the criteria?\n",
        "3. Can it suggest appropriate authority windows for different risk levels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load example ClipCard\n",
        "try:\n",
        "    with open('../../examples/test.clipcard.json', 'r') as f:\n",
        "        test_card = json.load(f)\n",
        "    print(\"Loaded test ClipCard:\")\n",
        "    print(json.dumps(test_card, indent=2))\n",
        "except FileNotFoundError:\n",
        "    print(\"No test ClipCard found.\")\n",
        "    test_card = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "### Test 1: Extract Kill Criteria\n",
        "\n",
        "Can we programmatically parse kill criteria?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_card:\n",
        "    kill_criteria = test_card.get('kill_criteria', [])\n",
        "    print(f\"Found {len(kill_criteria)} kill criteria:\\n\")\n",
        "    for i, criterion in enumerate(kill_criteria, 1):\n",
        "        print(f\"{i}. Condition: {criterion['condition']}\")\n",
        "        print(f\"   Action: {criterion['action']}\\n\")\n",
        "else:\n",
        "    print(\"No ClipCard available for testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "### Test 2: Scenario Evaluation\n",
        "\n",
        "Test if scenarios match kill criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test scenarios for DevOps/SRE\n",
        "scenarios = [\n",
        "    {\n",
        "        \"name\": \"Normal Operation\",\n",
        "        \"error_rate\": 0.003,\n",
        "        \"latency_p99\": 250,\n",
        "        \"mttr_hours\": 0.5,\n",
        "        \"expected_trigger\": False\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"High Error Rate\",\n",
        "        \"error_rate\": 0.015,\n",
        "        \"latency_p99\": 200,\n",
        "        \"mttr_hours\": 0.3,\n",
        "        \"expected_trigger\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"High Latency\",\n",
        "        \"error_rate\": 0.002,\n",
        "        \"latency_p99\": 600,\n",
        "        \"mttr_hours\": 0.4,\n",
        "        \"expected_trigger\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Slow Recovery\",\n",
        "        \"error_rate\": 0.004,\n",
        "        \"latency_p99\": 300,\n",
        "        \"mttr_hours\": 3.0,\n",
        "        \"expected_trigger\": True\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Testing scenarios against typical DevOps/SRE kill criteria:\\n\")\n",
        "print(\"Assumed thresholds:\")\n",
        "print(\"  - Error rate: > 0.01\")\n",
        "print(\"  - Latency p99: > 500ms\")\n",
        "print(\"  - MTTR: > 2.0 hours\\n\")\n",
        "\n",
        "results = []\n",
        "for scenario in scenarios:\n",
        "    # Evaluate against typical DevOps/SRE thresholds\n",
        "    error_trigger = scenario['error_rate'] > 0.01\n",
        "    latency_trigger = scenario['latency_p99'] > 500\n",
        "    mttr_trigger = scenario['mttr_hours'] > 2.0\n",
        "    \n",
        "    any_trigger = error_trigger or latency_trigger or mttr_trigger\n",
        "    match = any_trigger == scenario['expected_trigger']\n",
        "    \n",
        "    results.append({\n",
        "        'Scenario': scenario['name'],\n",
        "        'Error Trigger': error_trigger,\n",
        "        'Latency Trigger': latency_trigger,\n",
        "        'MTTR Trigger': mttr_trigger,\n",
        "        'Expected': scenario['expected_trigger'],\n",
        "        'Match': '\u2713' if match else '\u2717'\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.to_string(index=False))\n",
        "print(f\"\\nTest Accuracy: {df_results['Match'].value_counts().get('\u2713', 0)}/{len(results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "### Test 3: Authority Window Recommendations\n",
        "\n",
        "Generate appropriate authority windows based on risk factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_authority_window(impact, uncertainty, reversibility):\n",
        "    \"\"\"\n",
        "    Recommend authority window based on ClipCard risk factors\n",
        "    \n",
        "    Args:\n",
        "        impact: 1-5 scale\n",
        "        uncertainty: 1-5 scale\n",
        "        reversibility: 1-5 scale (1=easy to reverse, 5=irreversible)\n",
        "    \"\"\"\n",
        "    risk_score = impact * uncertainty\n",
        "    \n",
        "    if risk_score >= 20 or reversibility >= 5:\n",
        "        return {\n",
        "            \"scope_limit\": \"1% traffic\",\n",
        "            \"time_limit\": \"24h\",\n",
        "            \"auto_pause\": True,\n",
        "            \"justification\": \"Very high risk requires minimal exposure\"\n",
        "        }\n",
        "    elif risk_score >= 15 or reversibility >= 4:\n",
        "        return {\n",
        "            \"scope_limit\": \"5% traffic\",\n",
        "            \"time_limit\": \"7d\",\n",
        "            \"auto_pause\": True,\n",
        "            \"justification\": \"High risk requires controlled rollout\"\n",
        "        }\n",
        "    elif risk_score >= 10:\n",
        "        return {\n",
        "            \"scope_limit\": \"10% traffic\",\n",
        "            \"time_limit\": \"14d\",\n",
        "            \"auto_pause\": False,\n",
        "            \"justification\": \"Medium risk allows moderate exposure\"\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"scope_limit\": \"25% traffic\",\n",
        "            \"time_limit\": \"30d\",\n",
        "            \"auto_pause\": False,\n",
        "            \"justification\": \"Lower risk permits broader exposure\"\n",
        "        }\n",
        "\n",
        "# Test with example scenarios\n",
        "test_cases = [\n",
        "    {\"name\": \"High-risk service\", \"impact\": 5, \"uncertainty\": 4, \"reversibility\": 5},\n",
        "    {\"name\": \"Medium-risk API\", \"impact\": 4, \"uncertainty\": 3, \"reversibility\": 2},\n",
        "    {\"name\": \"Low-risk config change\", \"impact\": 2, \"uncertainty\": 2, \"reversibility\": 1}\n",
        "]\n",
        "\n",
        "print(\"Authority Window Recommendations:\\n\")\n",
        "for case in test_cases:\n",
        "    rec = recommend_authority_window(case['impact'], case['uncertainty'], case['reversibility'])\n",
        "    print(f\"{case['name']} (I={case['impact']}, U={case['uncertainty']}, R={case['reversibility']}):\")\n",
        "    print(f\"  Scope: {rec['scope_limit']}\")\n",
        "    print(f\"  Duration: {rec['time_limit']}\")\n",
        "    print(f\"  Auto-pause: {rec['auto_pause']}\")\n",
        "    print(f\"  Reason: {rec['justification']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "### Summary\n",
        "\n",
        "This notebook demonstrates that ClipCard data structures can be:\n",
        "- Programmatically parsed\n",
        "- Evaluated against scenarios\n",
        "- Used to generate risk-appropriate recommendations\n",
        "\n",
        "This enables AI agents to assist with ClipCard creation, validation, and decision support."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}